% !TEX program = XeLaTeX
% !TEX encoding = UTF-8
% @brief: group meeting slide
% @author: Zihang Xu
% @reference: https://github.com/latexstudio/HUST-Beamer-Theme
% @date: 2024-12-02

\documentclass{beamer}
\usepackage{ctex, hyperref}
\usepackage[T1]{fontenc}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{xeCJK}
\usepackage{HUST}

\author{徐梓航}
\title{Can Large Language Models Understand Real-World Complex Instructions?}
\subtitle{
    \scriptsize
    Qianyu He\scriptsize\texorpdfstring{$^{@FDU}$}{\textit{@FDU}}
    \normalfont
    , Jie Zeng\scriptsize\texorpdfstring{$^{@FDU}$}{\textit{@FDU}}
    \normalfont
    , et al. AAAI, 2024
}
\institute{华中科技大学计算机科学与技术学院}
\date{2024年12月02日}

\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}
\setCJKfamilyfont{hei}{SimHei}

\begin{frame}
    \titlepage
    \begin{figure}[htpb]
        % \begin{center}
        %     \includegraphics[width=0.2\linewidth]{images/templates/HUST_LOGO.png}
        % \end{center}
    \end{figure}
\end{frame}

\begin{frame}
    \tableofcontents[sectionstyle=show,subsectionstyle=show/shaded/hide,subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Introduction}

\begin{frame}{Can Large Language Models Understand Real-World Complex Instructions?}
    LLMs 难以处理复杂的指令，这些指令可以是需要多个任务和约束的复杂任务描述，也可以是包含长上下文、噪声、异构信息和多回合格式的复杂输入。
    \newline
    \newline
    由于这些特性，LLM常常忽略任务描述中的语义约束，生成错误的格式，违反长度或样本计数约束，并且对输入文本不忠实。
    \newline
    \newline
    现有的基准测试不足以评估LLMs对评估复杂指令的能力，为此，论文提出了CELLO (\textcolor{red}{C}ompl\textcolor{red}{E}x instruction understanding ability of \textcolor{red}{L}arge \textcolor{red}{L}anguage M\textcolor{red}{O}dels)。
\end{frame}

\subsection{Complex Instructions}

\begin{frame}
    Instruction generally consists of two parts:
    \begin{itemize}
        \item {Task description (mandatory)}
        \item {Input text (optional)}
    \end{itemize}

    Two categories of complex instructions:
    \begin{itemize}
        \item {complex task descriptions}
        \item {complex input}
    \end{itemize}
\end{frame}

\begin{frame}
    Regarding complex task descriptions, models need to undertake multiple tasks and there can be diverse restrictions describing the task:
    \begin{itemize}
        \item {semantics constraints}
        \item {format constraints}
        \item {quantity constraints}
    \end{itemize}
    Regarding complex input, the input text generally have:
    \begin{itemize}
        \item {long context}
        \item {noise}
        \item {error accumulation caused by pipeline method}
        \item {heterogeneous information (异构信息) \{e.g. a combination of structured and unstructured data\}}
        \item {in the form of multi-turn}
    \end{itemize}
\end{frame}

\begin{frame}
    \textcolor{red}{The complexity of real-world instructions accounts for prevalent errors observed in LLMs.}
    \newline
    LLMs may:
    \begin{itemize}
        \item {Ignore semantic constraints from task description.}
        \item {Generate answers in incorrect format.}
        \item {Violate the length or sample count constraints, especially when multiple tasks are required to be performed.}
        \item {Models can be unfaithful to the input text, especially when it is long, noisy, heterogeneous or in the form of multi-turn.}
    \end{itemize}
\end{frame}

\subsection{Existing Benchmarks}

\begin{frame}
    Existing benchmarks are insufficient for effectively assessing the ability of LLMs to understand complex instructions:
    \begin{itemize}
        \item {close-ended (封闭式)}
        \item {Contain common and simple instructions, which fail to mirror the complexity of real-world instructions.}
    \end{itemize}
    They only encompass isolated features:
    \begin{itemize}
        \item {count restriction}
        \item {semantic restriction}
        \item {long text understanding}
    \end{itemize}
    Real-world instructions comprehensively cover these features. Overall, none of the existing benchmarks systematically study the complex instructions understanding ability of LLMs.
\end{frame}

\section{Challenge}

\begin{frame}
    \begin{itemize}
        \item {Complex instructions in real-world scenarios are open-ended, thus the criteria commonly used for close-ended benchmarks are not suitable in such cases.}
        \item {Many studies adopt GPT4 evaluation for automated open-ended assessment, which introduces bias problems.}
        \item {The binary pass rate adopted by the benchmarks containing complex instructions is strict and coarsegrained, resulting in universally low scores for smaller LLM without discrimination.}
    \end{itemize}
\end{frame}

\begin{frame}{CELLO (\textcolor{red}{C}ompl\textcolor{red}{E}x instruction understanding ability of \textcolor{red}{L}arge \textcolor{red}{L}anguage M\textcolor{red}{O}dels)}
    \begin{itemize}
        \item {pioneer}
        \item {Propose a two-stage framework for constructing the evaluation dataset for LLM's complex instruction understanding.}
        \item {Design four evaluation criteria and corresponding automatic metrics for assessing LLMs' ability to understand complex instructions in a comprehensive and discriminative way.}
        \item {Tested the benchmark testing framework.}
    \end{itemize}
\end{frame}

\section{Methods}

\subsection{Related Work}

\begin{frame}{Related Works}
    \begin{itemize}
        \item {Evaluation for LLMs}
        \item {Complex Instruction Following}
        \item {Evaluation for Constrained Instructions}
    \end{itemize}
\end{frame}

\subsection{CELLO Benchmark}

\begin{frame}{Dataset Construction}
    Diversify the collected complex instructions through In-breadth Evolution and complicate the collected simple instructions through In-breadth Evolution.
    \newline
    \large\bfseries{Data Source and Selected Tasks}
    \newline
    \normalfont
    Include common NLP tasks found in existing benchmarks, while incorporating instructions with more complex task descriptions or input beyond those benchmarks.
\end{frame}

\begin{frame}{Dataset Construction}
    CELLO include nine tasks, classified into six categories:
    \begin{itemize}
        \item {Complex NLP Tasks}
            \begin{itemize}
                \item {long text summarization}
                \item {long text closed-domain question answering}
                \item {long text keywords extraction}
                \item {complex information extraction}
            \end{itemize}
        \item {Meta-prompt}
        \item {Planning}
        \item {Structured Input}
        \item {Well-guided Writing}
        \item {Detailed Brainstorming}
    \end{itemize}
\end{frame}

\begin{frame}{Dataset Construction}
    \large\bfseries{Data Evolution}
    \newline
    \normalfont
    The collected complex instructions have two limitations:
    \begin{itemize}
        \item {For those collected from real-world projects, the human-elaborated task descriptions are complex but alike.}
        \item {For those collected from usage logs, many simple instructions are not effectively utilized.}
    \end{itemize}
    Introduce two perspectives to evolve data, thereby achieving a more robust and reliable evaluation:
    \begin{itemize}
        \item {In-breadth Evolution (Aims to diversify the collected complex instructions)}
            \begin{itemize}
                \item {task description relocation}
                \item {task description paraphrasing}
                \item {task emulation}
            \end{itemize}

    \end{itemize}
\end{frame}

\begin{frame}{Dataset Construction}
    \begin{itemize}
        \item {In-depth Evolution (Aims to complicate the simple instructions to increase the data scale)}
            \begin{itemize}
                \item {constraints addition}
                \item {multi-round interaction}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Criteria}
    \newline
    \normalfont
    Encompass common errors made by models:
    \begin{itemize}
        \item {count limit}
        \item {answer format}
        \item {task-prescribed phrases}
        \item {input-dependent query}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Evaluation Metrics}
    \newline
    \normalfont
    每个样本$s_i$由指令$I_i$、模型答案$a_i$和给定的历史$h_i$组成，其中$h_i$是多轮对话中的前几轮$\{(I_0, a_0'), ..., (I_i−1', a_i−1')\}$。对于每个样本$s$，其每个标准的分数由多个子分数$C$组成，$C$是一个包含$\{c_1, c_2, ..., c_i\}$的集合。
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Count Limit}
    \newline
    \normalfont
    Four sub-scores:
    \begin{itemize}
        \item {word count score}
        \item {sentence count score}
        \item {sample count score}
        \item {revise score}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Answer Format}
    \newline
    \normalfont
    Two sub-scores:
    \begin{itemize}
        \item {parseability (模型输出是否可解析，取0或1)}
        \item {keywords (计算模型输出中包含的关键词数量后/总数)}
    \end{itemize}
    最终两者求均值。
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Input-dependent Query}
    \newline
    \normalfont
    Two sub-scores:
    \begin{itemize}
        \item {keywords($f_{keywords}(a_i, l_q)$, the scoring keywords $l_q$ are extracted from input text)}
        \item {COPYBLEU (值随着模型输出与输入文本相似度的增加而减少，即如果模型输出与输入文本过于相似，COPYBLEU的值会较低，从而对最终得分产生负面影响)}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Task-prescribed Phrases}
    \newline
    \normalfont
    The more mandatory phrases covered in the answers, the better the model follows complex instructions.
    \newline
    \newline
    Keywords($f_{keywords}(a_i, l_t)$) is applied where $l_t$ is the scoring keywords extracted from the task description.
\end{frame}

\begin{frame}{Evaluation of the Benchmark}
    根据四个标准，每个样本由三个annotators标记。具体地说，只有当至少两个annotators在标准计数限制和输出格式可解析性上达成一致时，我们才保留样本。对于涉及关键字覆盖率的标准，我们只保留至少两个annotators一致同意的关键字。
\end{frame}

\begin{frame}{Statistics of the Benchmark}
    \begin{itemize}
        \item {Dataset has two categories depending on whether the criteria are mainly in the task description or the input text.}
        \item {CELLO benchmark is the first to systematically test LLMs' ability to follow complex instructions, which are generally longer and more complex than other benchmarks.}
        \item {The tasks we cover are open-ended, which are more realistic and practical.}
        \item {Evaluation is also more objective and fine-grained.}
    \end{itemize}
\end{frame}

\section{Experiment}

\begin{frame}{Evaluated Models}
    These models are categorized into three groups:
    \begin{itemize}
        \item {Chinese-oriented Models (From Scratch, FS) \{Trained entirely from scratch using Chinese corpora\}}
        \item {Chinese-oriented Models (Continue Pretraining, CP) \{Continue pretraining on Chinese corpora utilizing an English-oriented base model\}}
        \item {English-oriented Models}
    \end{itemize}
\end{frame}

\begin{frame}{Task-categorized Performance}
    \begin{itemize}
        \item {General Comparisons}
            \begin{itemize}
                \item {Complex instruction comprehension is not language-dependent.}
                \item {There is a strong correlation between the ability to comprehend complex instructions and the instruction.}
            \end{itemize}
        \item {Complex Task Description}
            \begin{itemize}
                \item {The ability to understand complex task descriptions can transfer across different languages.}
                \item {The supported text context length does not significantly impact the ability to comprehend complex task descriptions.}
            \end{itemize}
        \item {Complex Input Text}
            \begin{itemize}
                \item {More Chinese training data assists the models in comprehending long and noisy Chinese texts.}
                \item {Within \textcolor{red}{the same model series}, larger scales generally improve performance, while longer supported context length can result in performance drops in many cases.}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Criteria-categorized Performance}
    \begin{itemize}
        \item {Regarding \textcolor{red}{Answer format}, the English-oriented Models significantly perform better than Chinese-oriented Models. This demonstrates the English-oriented Models' ability to follow few-shot examples and generate code, as well as partially explains why their complex instruction-following ability can transfer across languages.}
        \item {For \textcolor{red}{Task-prescribed phrases}, Chinese data helps the models understand Chinese semantic restrictions.}
        \item {Finally, the performance differences between models for \textcolor{red}{Count limit} criteria are not big compared to other criteria, which shows that the models have similar comprehension of numerical concepts.}
    \end{itemize}
\end{frame}

\begin{frame}{Comparisons between Benchmarks}
    \begin{itemize}
        \item {On benchmarks focusing on Chinese knowledge (C-eval, CMMLU, and GAOKAO), smaller models achieve similar or even better performance compared to GPT-3.5-turbo.}
        \item {On challenging benchmarks like complex reasoning (BBH, GSM8k) and programming ability (HumanEval), there is a lack of distinction between smaller models.}
    \end{itemize}
\end{frame}

\begin{frame}{Fine-grained Evaluation}
    \begin{itemize}
        \item {Different models have different strengths for different criteria.}
        \item {Different models also excel in specific tasks.}
    \end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}
    \begin{itemize}
        \item {complex instructions following ability of LLMs}
        \item {CELLO Benchmark}
        \item {Conduct extensive experiments to compare the performance of representative models.}
    \end{itemize}
\end{frame}

\section{Thoughts}

\begin{frame}{思考}
    \begin{itemize}
        \item {\bfseries{数据集的多样性和代表性：}\normalfont
            论文中提出的CELLO基准测试涵盖了多种复杂指令的特征，并从真实世界场景中构建了评估数据集。然而，数据集的多样性和代表性始终是一个可以进一步探讨的话题。未来的工作可以探索如何确保数据集覆盖更广泛的语言、地区和文化背景，以及如何平衡不同领域和任务类型的样本。}
        \item {\bfseries{评估标准的细化：}\normalfont
            尽管论文提出了四个评估标准（Count limit、Answer format、Task-prescribed phrases、Input-dependent query），但这些标准是否可以进一步细化，以便更精确地捕捉模型在处理复杂指令时的细微差异，是一个值得考虑的问题。}
        \item {\bfseries{模型的可解释性：}\normalfont
            论文主要关注模型的性能评估，但对于模型的决策过程和内部机制的可解释性讨论不多。未来的研究可以探索如何提高模型在处理复杂指令时的透明度和可解释性，以便更好地理解模型的行为。}
    \end{itemize}
\end{frame}

\begin{frame}{思考}
    \begin{itemize}
        \item {\bfseries{模型的适应性和泛化能力：}\normalfont
            论文中的实验主要关注模型在特定数据集上的表现。未来的研究可以探讨模型在面对新的、未见过的复杂指令时的适应性和泛化能力，以及如何通过持续学习或迁移学习来提高这些能力。}
        \item {\bfseries{多模态和跨领域指令的理解：}\normalfont
            随着多模态学习和跨领域应用的兴起，未来的研究可以考虑如何评估和提高模型在处理包含图像、声音等多种模态信息的复杂指令时的性能。}
        \item {\bfseries{实时性能和资源消耗：}\normalfont
            论文中的评估主要关注模型的准确性和完成度。在实际应用中，模型的实时性能和资源消耗（如计算时间、内存使用等）也是非常重要的考量因素。未来的工作可以探讨如何在保证性能的同时优化模型的效率。}
    \end{itemize}
\end{frame}

\section{References}

\begin{frame}[allowframebreaks]
    \nocite{*} % 引用 ref.bib 中的所有条目
    \bibliography{ref}
    \bibliographystyle{alpha}
    % 如果参考文献太多的话，可以像下面这样调整字体：
    % \tiny\bibliographystyle{alpha}
\end{frame}

\begin{frame}
    \begin{center}
        {\Huge\calligra Thanks!}
    \end{center}
\end{frame}

\end{document}
