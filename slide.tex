% this template mostly comes from Trinkle23897
% his github repo is https://github.com/Trinkle23897/THU-Beamer-Theme
% thanks a lot Trinkle!

\documentclass{beamer}
\usepackage{ctex, hyperref}
\usepackage[T1]{fontenc}

% other packages
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}

\author{徐梓航}
\title{组会}
\subtitle{第1次}
\institute{华中科技大学计算机科学与技术学院}
\date{2024年12月02日}
\usepackage{HUST}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}

\kaishu
\begin{frame}
    \titlepage
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[width=0.2\linewidth]{images/templates/HUST_LOGO.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{itemize}[<+->]
        \item {工欲善其事，必先利其器}
        \item {我首先改了一个\LaTeX{} Beamer模板用于今后的组会PPT}
        \item GitHub 项目地址位于 \url{https://github.com/M0rtzz/GroupMeetingSlide}
    \end{itemize}
\end{frame}

\begin{frame}
    \tableofcontents[sectionstyle=show,subsectionstyle=show/shaded/hide,subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Introduction}

\begin{frame}{Can Large Language Models Understand Real-World Complex Instructions?}

    LLMs 难以处理复杂的指令，这些指令可以是需要多个任务和约束的复杂任务描述，也可以是包含长上下文、噪声、异构信息和多回合格式的复杂输入。
    \newline
    \newline
    由于这些特性，LLM常常忽略任务描述中的语义约束，生成错误的格式，违反长度或样本计数约束，并且对输入文本不忠实。
    \newline
    \newline
    现有的基准测试不足以评估LLMs对评估复杂指令的能力，为此，论文提出了CELLO (\textcolor{red}{C}ompl\textcolor{red}{E}x instruction understanding ability of \textcolor{red}{L}arge \textcolor{red}{L}anguage M\textcolor{red}{O}dels)。

\end{frame}

\subsection{Complex Instructions}

\begin{frame}
    Instruction generally consists of two parts:
    \begin{itemize}
        \item {Task description (mandatory)}
        \item {Input text (optional)}
    \end{itemize}

    Two categories of complex instructions:
    \begin{itemize}
        \item {complex task descriptions}
        \item {complex input}
    \end{itemize}

\end{frame}

\begin{frame}

    Regarding complex task descriptions, models need to undertake multiple tasks and there can be diverse restrictions describing the task:
    \begin{itemize}
        \item {semantics constraints}
        \item {format constraints}
        \item {quantity constraints}
    \end{itemize}
    Regarding complex input, the input text generally have:
    \begin{itemize}
        \item {long context}
        \item {noise}
        \item {error accumulation caused by pipeline method}
        \item {heterogeneous information (异构信息) \{e.g. a combination of structured and unstructured data\}}
        \item {in the form of multi-turn}
    \end{itemize}
\end{frame}

\begin{frame}
    \textcolor{red}{The complexity of real-world instructions accounts for prevalent errors observed in LLMs.}
    \newline
    LLMs may:
    \begin{itemize}
        \item {ignore semantic constraints from task description}
        \item {generate answers in incorrect format}
        \item {violate the length or sample count constraints, especially when multiple tasks are required to be performed}
        \item {models can be unfaithful to the input text, especially when it is long, noisy, heterogeneous or in the form of multi-turn}
    \end{itemize}

\end{frame}

\subsection{Existing Benchmarks}

\begin{frame}
    Existing benchmarks are insufficient for effectively assessing the ability of LLMs to understand complex instructions:
    \begin{itemize}
        \item {close-ended (封闭式)}
        \item {contain common and simple instructions, which fail to mirror the complexity of real-world instructions}
    \end{itemize}
    They only encompass isolated features:
    \begin{itemize}
        \item {count restriction}
        \item {semantic restriction}
        \item {long text understanding}
    \end{itemize}
    Real-world instructions comprehensively cover these features. Overall, none of the existing benchmarks systematically study the complex instructions understanding ability of LLMs.
\end{frame}

\section{Challenge}

\begin{frame}
    \begin{itemize}
        \item {Complex instructions in real-world scenarios are open-ended, thus the criteria commonly used for close-ended benchmarks are not suitable in such cases.}
        \item {Many studies adopt GPT4 evaluation for automated open-ended assessment, which introduces bias problems.}
        \item {The binary pass rate adopted by the benchmarks containing complex instructions is strict and coarsegrained, resulting in universally low scores for smaller LLM without discrimination.}
    \end{itemize}
\end{frame}

\begin{frame}{CELLO (\textcolor{red}{C}ompl\textcolor{red}{E}x instruction understanding ability of \textcolor{red}{L}arge \textcolor{red}{L}anguage M\textcolor{red}{O}dels)}
    \begin{itemize}
        \item {pioneer}
        \item {Propose a two-stage framework for constructing the evaluation dataset for LLM’s complex instruction understanding.}
        \item {Design four evaluation criteria and corresponding automatic metrics for assessing LLMs’ ability to understand complex instructions in a comprehensive and discriminative way.}
        \item {Tested the benchmark testing framework.}
    \end{itemize}
\end{frame}

\section{Methods}

\subsection{Related Work}

\begin{frame}{Related Works}
    \begin{itemize}
        \item {Evaluation for LLMs}
        \item {Complex Instruction Following}
        \item {Evaluation for Constrained Instructions}
    \end{itemize}
\end{frame}

\subsection{CELLO Benchmark}

\begin{frame}{Dataset Construction}
    Diversify the collected complex instructions through In-breadth Evolution and complicate the collected simple instructions through In-breadth Evolution.
    \newline
    \large\bfseries{Data Source and Selected Tasks}
    \newline
    \normalfont
    Include common NLP tasks found in existing benchmarks, while incorporating instructions with more complex task descriptions or input beyond those benchmarks. CELLO include nine tasks, classified into six categories:
    \begin{itemize}
        \item {Complex NLP Tasks}
        \item {Meta-prompt}
        \item {Planning}
        \item {Structured Input}
        \item {Well-guided Writing}
        \item {Detailed Brainstorming}
    \end{itemize}
\end{frame}

\section{Experiment}

\begin{frame}
    \begin{itemize}
        \item 一月：完成文献调研
        \item 二月：复现并评测各种Beamer主题美观程度
        \item 三、四月：美化THU Beamer主题
        \item 五月：论文撰写
    \end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}
    \begin{itemize}
        \item 一月：完成文献调研
        \item 二月：复现并评测各种Beamer主题美观程度
        \item 三、四月：美化THU Beamer主题
        \item 五月：论文撰写
    \end{itemize}
\end{frame}

\section{Thoughts}

\begin{frame}
    \begin{itemize}
        \item 一月：完成文献调研
        \item 二月：复现并评测各种Beamer主题美观程度
        \item 三、四月：美化THU Beamer主题
        \item 五月：论文撰写
    \end{itemize}
\end{frame}

\section{References}

\begin{frame}[allowframebreaks]
    \nocite{*} % 引用 ref.bib 中的所有条目
    \bibliography{ref}
    \bibliographystyle{alpha}
    % 如果参考文献太多的话，可以像下面这样调整字体：
    % \tiny\bibliographystyle{alpha}
\end{frame}

\begin{frame}
    \begin{center}
        {\Huge\calligra Thanks!}
    \end{center}
\end{frame}

\end{document}
