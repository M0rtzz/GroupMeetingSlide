% !TEX program = XeLaTeX
% !TEX encoding = UTF-8
% @brief: group meeting slide
% @author: Zihang Xu
% @reference:
% @date: 2024-12-02

\documentclass{beamer}
\usepackage{ctex, hyperref}
\usepackage[T1]{fontenc}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{HUST}

\author{徐梓航}
\title{组会}
\subtitle{第1次}
\institute{华中科技大学计算机科学与技术学院}
\date{2024年12月02日}

\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}
\kaishu

\begin{frame}
    \titlepage
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[width=0.2\linewidth]{images/templates/HUST_LOGO.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{itemize}[<+->] % 当然，除了这样，手动在里面插 \pause 也行
        \item {工欲善其事，必先利其器}
        \item {我首先改了一个\LaTeX{} Beamer模板用于今后的组会PPT}
        \item GitHub 项目地址位于 \url{https://github.com/M0rtzz/GroupMeetingSlide}
    \end{itemize}
\end{frame}

\begin{frame}
    \tableofcontents[sectionstyle=show,subsectionstyle=show/shaded/hide,subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Introduction}

\begin{frame}{Can Large Language Models Understand Real-World Complex Instructions?}
    LLMs 难以处理复杂的指令，这些指令可以是需要多个任务和约束的复杂任务描述，也可以是包含长上下文、噪声、异构信息和多回合格式的复杂输入。
    \newline
    \newline
    由于这些特性，LLM常常忽略任务描述中的语义约束，生成错误的格式，违反长度或样本计数约束，并且对输入文本不忠实。
    \newline
    \newline
    现有的基准测试不足以评估LLMs对评估复杂指令的能力，为此，论文提出了CELLO (\textcolor{red}{C}ompl\textcolor{red}{E}x instruction understanding ability of \textcolor{red}{L}arge \textcolor{red}{L}anguage M\textcolor{red}{O}dels)。

\end{frame}

\subsection{Complex Instructions}

\begin{frame}
    Instruction generally consists of two parts:
    \begin{itemize}
        \item {Task description (mandatory)}
        \item {Input text (optional)}
    \end{itemize}

    Two categories of complex instructions:
    \begin{itemize}
        \item {complex task descriptions}
        \item {complex input}
    \end{itemize}

\end{frame}

\begin{frame}
    Regarding complex task descriptions, models need to undertake multiple tasks and there can be diverse restrictions describing the task:
    \begin{itemize}
        \item {semantics constraints}
        \item {format constraints}
        \item {quantity constraints}
    \end{itemize}
    Regarding complex input, the input text generally have:
    \begin{itemize}
        \item {long context}
        \item {noise}
        \item {error accumulation caused by pipeline method}
        \item {heterogeneous information (异构信息) \{e.g. a combination of structured and unstructured data\}}
        \item {in the form of multi-turn}
    \end{itemize}
\end{frame}

\begin{frame}
    \textcolor{red}{The complexity of real-world instructions accounts for prevalent errors observed in LLMs.}
    \newline
    LLMs may:
    \begin{itemize}
        \item {ignore semantic constraints from task description}
        \item {generate answers in incorrect format}
        \item {violate the length or sample count constraints, especially when multiple tasks are required to be performed}
        \item {models can be unfaithful to the input text, especially when it is long, noisy, heterogeneous or in the form of multi-turn}
    \end{itemize}

\end{frame}

\subsection{Existing Benchmarks}

\begin{frame}
    Existing benchmarks are insufficient for effectively assessing the ability of LLMs to understand complex instructions:
    \begin{itemize}
        \item {close-ended (封闭式)}
        \item {contain common and simple instructions, which fail to mirror the complexity of real-world instructions}
    \end{itemize}
    They only encompass isolated features:
    \begin{itemize}
        \item {count restriction}
        \item {semantic restriction}
        \item {long text understanding}
    \end{itemize}
    Real-world instructions comprehensively cover these features. Overall, none of the existing benchmarks systematically study the complex instructions understanding ability of LLMs.
\end{frame}

\section{Challenge}

\begin{frame}
    \begin{itemize}
        \item {Complex instructions in real-world scenarios are open-ended, thus the criteria commonly used for close-ended benchmarks are not suitable in such cases.}
        \item {Many studies adopt GPT4 evaluation for automated open-ended assessment, which introduces bias problems.}
        \item {The binary pass rate adopted by the benchmarks containing complex instructions is strict and coarsegrained, resulting in universally low scores for smaller LLM without discrimination.}
    \end{itemize}
\end{frame}

\begin{frame}{CELLO (\textcolor{red}{C}ompl\textcolor{red}{E}x instruction understanding ability of \textcolor{red}{L}arge \textcolor{red}{L}anguage M\textcolor{red}{O}dels)}
    \begin{itemize}
        \item {pioneer}
        \item {Propose a two-stage framework for constructing the evaluation dataset for LLM's complex instruction understanding.}
        \item {Design four evaluation criteria and corresponding automatic metrics for assessing LLMs' ability to understand complex instructions in a comprehensive and discriminative way.}
        \item {Tested the benchmark testing framework.}
    \end{itemize}
\end{frame}

\section{Methods}

\subsection{Related Work}

\begin{frame}{Related Works}
    \begin{itemize}
        \item {Evaluation for LLMs}
        \item {Complex Instruction Following}
        \item {Evaluation for Constrained Instructions}
    \end{itemize}
\end{frame}

\subsection{CELLO Benchmark}

\begin{frame}{Dataset Construction}
    Diversify the collected complex instructions through In-breadth Evolution and complicate the collected simple instructions through In-breadth Evolution.
    \newline
    \large\bfseries{Data Source and Selected Tasks}
    \newline
    \normalfont
    Include common NLP tasks found in existing benchmarks, while incorporating instructions with more complex task descriptions or input beyond those benchmarks.
\end{frame}

\begin{frame}{Dataset Construction}
    CELLO include nine tasks, classified into six categories:
    \begin{itemize}
        \item {Complex NLP Tasks}
            \begin{itemize}
                \item {long text summarization}
                \item {long text closed-domain question answering}
                \item {long text keywords extraction}
                \item {complex information extraction}
            \end{itemize}
        \item {Meta-prompt}
        \item {Planning}
        \item {Structured Input}
        \item {Well-guided Writing}
        \item {Detailed Brainstorming}
    \end{itemize}
\end{frame}

\begin{frame}{Dataset Construction}
    \large\bfseries{Data Evolution}
    \newline
    \normalfont
    The collected complex instructions have two limitations:
    \begin{itemize}
        \item {For those collected from real-world projects, the human-elaborated task descriptions are complex but alike.}
        \item {For those collected from usage logs, many simple instructions are not effectively utilized.}
    \end{itemize}
    Introduce two perspectives to evolve data, thereby achieving a more robust and reliable evaluation:
    \begin{itemize}
        \item {In-breadth Evolution (Aims to diversify the collected complex instructions)}
            \begin{itemize}
                \item {task description relocation}
                \item {task description paraphrasing}
                \item {task emulation}
            \end{itemize}

    \end{itemize}
\end{frame}

\begin{frame}{Dataset Construction}
    \begin{itemize}
        \item {In-depth Evolution (Aims to complicate the simple instructions to increase the data scale)}
            \begin{itemize}
                \item {constraints addition}
                \item {multi-round interaction}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Criteria}
    \newline
    \normalfont
    Encompass common errors made by models:
    \begin{itemize}
        \item {count limit}
        \item {answer format}
        \item {task-prescribed phrases}
        \item {input-dependent query}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Evaluation Metrics}
    \newline
    \normalfont
    每个样本$s_i$由指令$I_i$、模型答案$a_i$和给定的历史$h_i$组成，其中$h_i$是多轮对话中的前几轮$\{(I_0, a_0'), ..., (I_i−1', a_i−1')\}$。对于每个样本$s$，其每个标准的分数由多个子分数$C$组成，$C$是一个包含$\{c_1, c_2, ..., c_i\}$的集合。
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Count Limit}
    \newline
    \normalfont
    Four sub-scores:
    \begin{itemize}
        \item {word count score}
        \item {sentence count score}
        \item {sample count score}
        \item {revise score}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Answer Format}
    \newline
    \normalfont
    Two sub-scores:
    \begin{itemize}
        \item {parseability (模型输出是否可解析，取0或1)}
        \item {keywords (计算模型输出中包含的关键词数量后/总数)}
    \end{itemize}
    最终两者求均值。
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Input-dependent Query}
    \newline
    \normalfont
    Two sub-scores:
    \begin{itemize}
        \item {keywords($f_{keywords}(a_i, l_q)$, the scoring keywords $l_q$ are extracted from input text)}
        \item {COPYBLEU (值随着模型输出与输入文本相似度的增加而减少，即如果模型输出与输入文本过于相似，COPYBLEU的值会较低，从而对最终得分产生负面影响)}
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation System}
    \large\bfseries{Task-prescribed Phrases}
    \newline
    \normalfont
    The more mandatory phrases covered in the answers, the better the model follows complex instructions.
    \newline
    \newline
    Keywords($f_{keywords}(a_i, l_t)$) is applied where $l_t$ is the scoring keywords extracted from the task description.
\end{frame}

\begin{frame}{Evaluation of the Benchmark}
    根据四个标准，每个样本由三个annotators标记。具体地说，只有当至少两个annotators在标准计数限制和输出格式可解析性上达成一致时，我们才保留样本。对于涉及关键字覆盖率的标准，我们只保留至少两个annotators一致同意的关键字。
\end{frame}

\begin{frame}{Statistics of the Benchmark}
    \begin{itemize}
        \item {Dataset has two categories depending on whether the criteria are mainly in the task description or the input text.}
        \item {CELLO benchmark is the first to systematically test LLMs' ability to follow complex instructions, which are generally longer and more complex than other benchmarks}
        \item {The tasks we cover are open-ended, which are more realistic and practical.}
        \item {Evaluation is also more objective and fine-grained.}
    \end{itemize}
\end{frame}

\section{Experiment}

\begin{frame}{Evaluated Models}
    These models are categorized into three groups:
    \begin{itemize}
        \item {Chinese-oriented Models (From Scratch, FS) \{Trained entirely from scratch using Chinese corpora\}}
        \item {Chinese-oriented Models (Continue Pretraining, CP) \{Continue pretraining on Chinese corpora utilizing an English-oriented base model\}}
        \item {English-oriented Models}
    \end{itemize}
\end{frame}

\begin{frame}{Task-categorized Performance}
    \begin{itemize}
        \item {General Comparisons}
            \begin{itemize}
                \item {Complex instruction comprehension is not language-dependent.}
                \item {There is a strong correlation between the ability to comprehend complex instructions and the instruction.}
            \end{itemize}
        \item {Complex Task Description}
            \begin{itemize}
                \item {The ability to understand complex task descriptions can transfer across different languages.}
                \item {The supported text context length does not significantly impact the ability to comprehend complex task descriptions.}
            \end{itemize}
        \item {Complex Input Text}
            \begin{itemize}
                \item {More Chinese training data assists the models in comprehending long and noisy Chinese texts.}
                \item {Within \textcolor{red}{the same model series}, larger scales generally improve performance, while longer supported context length can result in performance drops in many cases.}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Criteria-categorized Performance}
    \begin{itemize}
        \item {Regarding \textcolor{red}{Answer format}, the English-oriented Models significantly perform better than Chinese-oriented Models. This demonstrates the English-oriented Models' ability to follow few-shot examples and generate code, as well as partially explains why their complex instruction-following ability can transfer across languages.}
        \item {For \textcolor{red}{Task-prescribed phrases}, Chinese data helps the models understand Chinese semantic restrictions.}
        \item {Finally, the performance differences between models for \textcolor{red}{Count limit} criteria are not big compared to other criteria, which shows that the models have similar comprehension of numerical concepts.}
    \end{itemize}
\end{frame}

\begin{frame}{Comparisons between Benchmarks}
    \begin{itemize}
        \item {On benchmarks focusing on Chinese knowledge (C-eval, CMMLU, and GAOKAO), smaller models achieve similar or even better performance compared to GPT-3.5-turbo.}
        \item {On challenging benchmarks like complex reasoning (BBH, GSM8k) and programming ability (HumanEval), there is a lack of distinction between smaller models.}
    \end{itemize}
\end{frame}

\begin{frame}{Fine-grained Evaluation}
    \begin{itemize}
        \item {Different models have different strengths for different criteria.}
        \item {Different models also excel in specific tasks.}
    \end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}
    \begin{itemize}
        \item {complex instructions following ability of LLMs}
        \item {CELLO Benchmark}
        \item {Conduct extensive experiments to compare the performance of representative models.}
    \end{itemize}
\end{frame}

\section{Thoughts}

\begin{frame}
    \begin{itemize}
        \item 一月：完成文献调研
        \item 二月：复现并评测各种Beamer主题美观程度
        \item 三、四月：美化THU Beamer主题
        \item 五月：论文撰写
    \end{itemize}
\end{frame}

\section{References}

\begin{frame}[allowframebreaks]
    \nocite{*} % 引用 ref.bib 中的所有条目
    \bibliography{ref}
    \bibliographystyle{alpha}
    % 如果参考文献太多的话，可以像下面这样调整字体：
    % \tiny\bibliographystyle{alpha}
\end{frame}

\begin{frame}
    \begin{center}
        {\Huge\calligra Thanks!}
    \end{center}
\end{frame}

\end{document}
